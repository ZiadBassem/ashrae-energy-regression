{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1  Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import joblib, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use relative paths so it works in Kaggle environment or locally\n",
    "data_path = \"../data/\"\n",
    "train       = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "test        = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "building    = pd.read_csv(os.path.join(data_path, \"building_metadata.csv\"))\n",
    "weather_tr  = pd.read_csv(os.path.join(data_path, \"weather_train.csv\"))\n",
    "weather_te  = pd.read_csv(os.path.join(data_path, \"weather_test.csv\"))\n",
    "\n",
    "# Keep electricity only\n",
    "train = train[train['meter'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weather(df, weather):\n",
    "    df = df.merge(building, on=\"building_id\", how=\"left\")\n",
    "    df = df.merge(weather, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
    "    return df\n",
    "\n",
    "train_full = merge_weather(train, weather_tr)\n",
    "test_full  = merge_weather(test,  weather_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Feature Engineering & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_full, test_full]:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['month']       = df['timestamp'].dt.month\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['is_weekend']  = df['day_of_week'].isin([5,6]).astype(int)\n",
    "    # Clean weather anomalies\n",
    "    df['precip_depth_1_hr'] = df['precip_depth_1_hr'].clip(lower=0).fillna(0)\n",
    "    df['air_temperature']   = df['air_temperature'].fillna(df['air_temperature'].median())\n",
    "    df['dew_temperature']   = df['dew_temperature'].fillna(df['dew_temperature'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4  Aggregate to Daily (level matching training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_daily(df):\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "\n",
    "    # Base aggregation for weather variables\n",
    "    agg_dict = {\n",
    "        'air_temperature': 'mean',\n",
    "        'dew_temperature': 'mean',\n",
    "        'wind_speed': 'mean',\n",
    "        'cloud_coverage': 'mean',\n",
    "        'precip_depth_1_hr': 'sum',\n",
    "        'sea_level_pressure': 'mean'\n",
    "    }\n",
    "\n",
    "    # Add target only if present (training set)\n",
    "    if 'meter_reading' in df.columns:\n",
    "        agg_dict['meter_reading'] = 'mean'\n",
    "\n",
    "    daily = (\n",
    "        df.groupby(['building_id', 'site_id', 'primary_use',\n",
    "                    'square_feet', 'year_built', 'floor_count',\n",
    "                    'month', 'day_of_week', 'is_weekend', 'date'])\n",
    "          .agg(agg_dict)\n",
    "          .reset_index()\n",
    "    )\n",
    "    return daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = aggregate_daily(train_full)\n",
    "test_d  = aggregate_daily(test_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Feature Lists and Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['square_feet','year_built','floor_count','air_temperature',\n",
    "                'dew_temperature','wind_speed','cloud_coverage',\n",
    "                'precip_depth_1_hr','sea_level_pressure',\n",
    "                'month','day_of_week','is_weekend']\n",
    "cat_features = ['primary_use','site_id']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts after cleaning:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# === FINAL CLEANING SAFETY NET ===\n",
    "import numpy as np\n",
    "\n",
    "def clean_numeric(df):\n",
    "    num_cols = ['square_feet','year_built','floor_count',\n",
    "                'air_temperature','dew_temperature',\n",
    "                'wind_speed','cloud_coverage',\n",
    "                'precip_depth_1_hr','sea_level_pressure',\n",
    "                'month','day_of_week','is_weekend']\n",
    "    \n",
    "    # Replace non‑finite values with NaN\n",
    "    df[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Specific fixes\n",
    "    df['precip_depth_1_hr'] = df['precip_depth_1_hr'].clip(lower=0)\n",
    "    df['air_temperature']   = df['air_temperature'].clip(lower=-50, upper=60)\n",
    "    df['dew_temperature']   = df['dew_temperature'].clip(lower=-50, upper=60)\n",
    "    df['sea_level_pressure']= df['sea_level_pressure'].clip(lower=800, upper=1100)\n",
    "    df['wind_speed']        = df['wind_speed'].clip(lower=0, upper=100)\n",
    "    df['cloud_coverage']    = df['cloud_coverage'].clip(lower=0, upper=10)\n",
    "    \n",
    "    # Fill residual NaNs with column median\n",
    "    df[num_cols] = df[num_cols].apply(lambda c: c.fillna(c.median()))\n",
    "\n",
    "    return df\n",
    "\n",
    "train_d = clean_numeric(train_d)\n",
    "test_d  = clean_numeric(test_d)\n",
    "\n",
    "print(\"NaN counts after cleaning:\")\n",
    "print(train_d.isna().sum()[train_d.isna().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6  Train Final Model (using Cleaned Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved ✓ → ../outputs/final_elasticnet_poly.pkl\n"
     ]
    }
   ],
   "source": [
    "import os, joblib\n",
    "\n",
    "os.makedirs(\"../outputs\", exist_ok=True)   # create folder if it doesn't exist\n",
    "joblib.dump(poly_enet, \"../outputs/final_elasticnet_poly.pkl\")\n",
    "print(\"Model trained and saved ✓ → ../outputs/final_elasticnet_poly.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved ✓\n"
     ]
    }
   ],
   "source": [
    "X_train = train_d.drop(columns=['meter_reading','date'])\n",
    "y_train = np.log1p(train_d['meter_reading'].clip(lower=0))  # log1p stabilization\n",
    "\n",
    "poly_enet = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('model', ElasticNet(alpha=0.001, l1_ratio=0.2, max_iter=20000, random_state=42))\n",
    "])\n",
    "\n",
    "poly_enet.fit(X_train, y_train)\n",
    "joblib.dump(poly_enet, \"../outputs/final_elasticnet_poly.pkl\")\n",
    "print(\"Model trained and saved ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7  Inference on Kaggle Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_d.drop(columns=['date'])\n",
    "pred_log = poly_enet.predict(X_test)\n",
    "pred_kwh = np.expm1(pred_log)\n",
    "pred_kwh = np.clip(pred_kwh, 0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8  Building Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv created: (168630, 2)\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"row_id\": test_d.index,\n",
    "    \"meter_reading\": pred_kwh\n",
    "})\n",
    "submission.to_csv(\"../outputs/submission.csv\", index=False)\n",
    "print(\"submission.csv created:\", submission.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "prev_pub_hash": "9c22683913f8ca448d5398e8f650d831d71c0004162f9ca83962b547ed3dfda0"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
